import abc
import time
import functools
import uuid
import requests
import sounddevice as sd
import wave
import os
import json
from datetime import datetime
import assemblyai as aai
#from elevenlabs import play
#from elevenlabs.client import ElevenLabs
from PIL import Image
import subprocess
from modules.constants import (
    OPENAI_IMG_AGENT_DIR,
    ELEVEN_LABS_CRINGE_VOICE,
    ELEVEN_LABS_PRIMARY_SOLID_VOICE,
)
from modules.simple_llm import build_mini_model, build_new_gpt4o, prompt
from dotenv import load_dotenv
import openai
from groq import Groq

# New imports for gTTS and speech recognition
from gtts import gTTS
import io
from pydub import AudioSegment
from pydub.playback import play # as pydub_play
#import speech_recognition as sr

from modules.typings import (
    ConvertImageParams,
    GenerateImageParams,
    ImageRatio,
    Style,
    ResizeImageParams,
    OpenImageDirParams,
)


class PersonalAssistantFramework(abc.ABC):
    @staticmethod
    def timeit_decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            result = func(*args, **kwargs)
            end_time = time.time()
            duration = round(end_time - start_time, 2)
            print(
                f"⏰ {args[0].__class__.__name__} - {func.__name__}() took {duration:.2f} seconds"
            )

            json_file = f"{args[0].__class__.__name__}_time_table.json"

            # Read existing data or create an empty list
            if os.path.exists(json_file):
                with open(json_file, "r") as file:
                    try:
                        data = json.load(file)
                    except json.JSONDecodeError:
                        data = []
            else:
                data = []

            # Create new time record
            time_record = {
                "assistant": args[0].__class__.__name__,
                "function": func.__name__,
                "duration": f"{duration:.2f}",
                "position": 0,  # New entry always at the top
            }

            # Update positions of existing records
            for record in data:
                record["position"] += 1

            # Insert new record at the beginning
            data.insert(0, time_record)

            # Sort data by position
            data.sort(key=lambda x: x["position"])

            # Write updated data back to file
            with open(json_file, "w") as file:
                json.dump(data, file, indent=2)

            return result

        return wrapper

    @abc.abstractmethod
    def setup(self):
        pass

    @abc.abstractmethod
    def transcribe(self, file_path):
        pass

    @abc.abstractmethod
    def speak(self, text: str):
        pass

    @abc.abstractmethod
    def think(self, prompt: str) -> str:
        pass


class AssElevenPAF(PersonalAssistantFramework):
    def setup(self):
        # AssemblyAI setup (commented out)
        # aai.settings.api_key = os.getenv("ASSEMBLYAI_API_KEY")

        # Groq setup
        self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))

        # OpenAI setup
        openai.api_key = os.getenv("OPENAI_API_KEY")

        self.llm_model = build_mini_model()

    @PersonalAssistantFramework.timeit_decorator
    def generate_voice_audio(self, text: str):
        # ElevenLabs version (commented out)
        # audio = generate(text=text, voice=self.eleven_voice, model="eleven_turbo_v2")
        # return audio

        # gTTS version
        tts = gTTS(text=text, lang='en')
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        fp.seek(0)
        return fp.read()

    @PersonalAssistantFramework.timeit_decorator
    def transcribe(self, file_path):
        # AssemblyAI transcription (commented out)
        # transcriber = aai.Transcriber()
        # transcript = transcriber.transcribe(file_path)
        # return transcript.text

        # Groq transcription
        with open(file_path, "rb") as audio_file:
            transcript = self.groq_client.audio.transcriptions.create(
                model="whisper-large-v3-turbo",  # Using the fastest multilingual model
                file=audio_file,
                response_format="text",
                # Optional parameters:
                # language="en",  # Uncomment and set if you know the language
                # temperature=0.0,  # Adjust between 0 and 1 for output control
                # prompt="Your context or specific words here",  # Uncomment and set if needed
            )
        return transcript

        # OpenAI transcription (commented out)
        # with open(file_path, "rb") as audio_file:
        #     transcript = openai.Audio.transcribe("whisper-1", audio_file)
        # return transcript["text"]

        # Free option: SpeechRecognition with Google Speech Recognition (commented out)
        # recognizer = sr.Recognizer()
        # with sr.AudioFile(file_path) as source:
        #     audio_data = recognizer.record(source)
        #     try:
        #         text = recognizer.recognize_google(audio_data)
        #         return text
        #     except sr.UnknownValueError:
        #         return "Speech Recognition could not understand the audio"
        #     except sr.RequestError as e:
        #         return f"Could not request results from Speech Recognition service; {e}"

    def speak(self, text: str, speed: float = 1.0):
        # Generate audio bytes using gTTS
        audio_bytes = self.generate_voice_audio(text)
        
        # Convert bytes to AudioSegment
        audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format="mp3")
        
        # Speed up the audio
        if speed != 1.0:
            audio = audio.speedup(playback_speed=speed)
        
        # Play the audio
        play(audio)

    @PersonalAssistantFramework.timeit_decorator
    def think(self, thought: str) -> str:
        return prompt(self.llm_model, thought)


class OpenAIPAF(PersonalAssistantFramework):
    def setup(self):
        openai.api_key = os.getenv("OPENAI_API_KEY")
        self.llm_model = build_mini_model()

    @PersonalAssistantFramework.timeit_decorator
    def transcribe(self, file_path):
        with open(file_path, "rb") as audio_file:
            transcript = openai.audio.transcriptions.create(
                model="whisper-1",  # this points to whisper v2 - Free OpenSource. See Docs (https://platform.openai.com/docs/api-reference/audio/createTranscription)
                file=audio_file,
            )
        return transcript.text

    @PersonalAssistantFramework.timeit_decorator
    def generate_voice_audio(self, text: str):
        # OPTION 1: OpenAI TTS (commented out)
        # response = openai.audio.speech.create(
        #     model="tts-1-hd", voice="shimmer", input=text, response_format="aac"
        # )

        # OPTION 2: Free gTTS version
        tts = gTTS(text=text, lang='en')
        fp = io.BytesIO()
        tts.write_to_fp(fp)
        fp.seek(0)
        return fp.read()

    def speak(self, text: str, speed: float = 1.5):
        # OPTION 1: OpenAI TTS (commented out)
        # audio = self.generate_voice_audio(text)
        # play(audio)

        # OPTION 2: Free gTTS version
        # Generate audio bytes using gTTS
        audio_bytes = self.generate_voice_audio(text)
        # Convert bytes to AudioSegment
        audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format="mp3")
        # Speed up the audio
        audio = audio.speedup(playback_speed=speed)
        # Play the audio
        play(audio)

    @PersonalAssistantFramework.timeit_decorator
    def think(self, thought: str) -> str:
        return prompt(self.llm_model, thought)


class GroqElevenPAF(PersonalAssistantFramework):
    def setup(self):
        self.groq_client = Groq()
        self.elevenlabs_client = ElevenLabs(api_key=os.getenv("ELEVEN_API_KEY"))
        self.llm_model = build_mini_model()

    @PersonalAssistantFramework.timeit_decorator
    def transcribe(self, file_path):
        with open(file_path, "rb") as file:
            transcription = self.groq_client.audio.transcriptions.create(
                file=(file_path, file.read()),
                model="distil-whisper-large-v3-en",
                response_format="text",
            )
        return str(transcription)

    @PersonalAssistantFramework.timeit_decorator
    def generate_voice_audio(self, text: str):
        audio_generator = self.elevenlabs_client.generate(
            text=text,
            voice=ELEVEN_LABS_PRIMARY_SOLID_VOICE,
            model="eleven_turbo_v2_5",
            stream=False,
        )
        audio_bytes = b"".join(list(audio_generator))
        return audio_bytes

    def speak(self, text: str):
        audio = self.generate_voice_audio(text)
        play(audio)

    @PersonalAssistantFramework.timeit_decorator
    def think(self, thought: str) -> str:
        return prompt(self.llm_model, thought)


class OpenAISuperPAF(OpenAIPAF):
    def setup(self):
        super().setup()
        openai.api_key = os.getenv("OPENAI_API_KEY")
        self.weak_model = build_mini_model()
        self.download_directory = os.path.join(os.getcwd(), OPENAI_IMG_AGENT_DIR)
        if not os.path.exists(self.download_directory):
            os.makedirs(self.download_directory)

    def generate_image(self, generate_image_params: GenerateImageParams) -> bool:

        # handle defaults
        if generate_image_params.image_ratio is None:
            generate_image_params.image_ratio = ImageRatio.SQUARE
        if generate_image_params.quality is None:
            generate_image_params.quality = "hd"
        if generate_image_params.style is None:
            generate_image_params.style = Style.NATURAL

        client = openai.OpenAI()
        subdirectory = os.path.join(self.download_directory)
        if not os.path.exists(subdirectory):
            os.makedirs(subdirectory)

        for index, prompt in enumerate(generate_image_params.prompts):
            print(f"🖼️ Generating image {index + 1} with prompt: {prompt}")
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size=generate_image_params.image_ratio.value,
                quality=generate_image_params.quality,
                n=1,
                style=generate_image_params.style.value,
            )
            image_url = response.data[0].url
            image_response = requests.get(image_url)
            image_path = os.path.join(subdirectory, f"version_{index}.png")
            with open(image_path, "wb") as file:
                file.write(image_response.content)

        return True

    def convert_image(self, convert_image_params: ConvertImageParams) -> bool:
        subdirectory = os.path.join(self.download_directory)
        if not os.path.exists(subdirectory):
            os.makedirs(subdirectory)

        for index in convert_image_params.version_numbers:
            input_path = os.path.join(subdirectory, f"version_{index}.png")
            if not os.path.exists(input_path):
                print(f"🟡 Warning: File {input_path} does not exist. Skipping.")
                continue

            output_path = os.path.join(
                subdirectory, f"version_{index}.{convert_image_params.image_format}"
            )

            try:
                with Image.open(input_path) as img:
                    img.save(
                        output_path,
                        format=convert_image_params.image_format.value.upper(),
                    )
                print(f"🖼️ Converted {input_path} to {output_path}")
            except Exception as e:
                print(f"Error converting {input_path}: {str(e)}")
                return False

        return True

    def resize_image(self, resize_image_params: ResizeImageParams) -> bool:
        subdirectory = os.path.join(self.download_directory)
        if not os.path.exists(subdirectory):
            os.makedirs(subdirectory)

        for index in resize_image_params.version_numbers:
            input_path = os.path.join(subdirectory, f"version_{index}.png")
            if not os.path.exists(input_path):
                print(f"🟡 Warning: File {input_path} does not exist. Skipping.")
                continue

            output_path = os.path.join(
                subdirectory,
                f"version_{index}_resized_w{resize_image_params.width}_h{resize_image_params.height}.png",
            )

            try:
                with Image.open(input_path) as img:
                    resized_img = img.resize(
                        (resize_image_params.width, resize_image_params.height)
                    )
                    resized_img.save(output_path)
                print(f"🖼️ Resized {input_path} to {output_path}")
            except Exception as e:
                print(f"Error resizing {input_path}: {str(e)}")
                return False

        return True

    def open_image_directory(self, open_image_dir_params: OpenImageDirParams) -> bool:
        try:
            if os.name == "nt":  # For Windows
                os.startfile(self.download_directory)
            elif os.name == "posix":  # For macOS and Linux
                subprocess.call(["open", self.download_directory])
            print(f"📂 Opened image directory: {self.download_directory}")
            return True
        except Exception as e:
            print(f"Error opening image directory: {str(e)}")
            return False

    @PersonalAssistantFramework.timeit_decorator
    def think(self, thought: str) -> str:
        client = openai.OpenAI()
        completion = client.beta.chat.completions.parse(
            model="gpt-4o-2024-08-06",
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": thought},
            ],
            tools=[
                openai.pydantic_function_tool(GenerateImageParams),
                openai.pydantic_function_tool(ConvertImageParams),
                openai.pydantic_function_tool(ResizeImageParams),
                openai.pydantic_function_tool(OpenImageDirParams),
            ],
        )

        message = completion.choices[0].message

        if message.tool_calls:

            tool_call = message.tool_calls[0]

            pretty_parsed_arguments = (
                tool_call.function.parsed_arguments.model_dump_json(indent=2)
            )

            print(
                f"""Tool call found: '{tool_call.function.name}(
{pretty_parsed_arguments}
)'. 
Calling..."""
            )

            success = False

            tool_call_success_prompt = f"Quickly let your human companion know that you've run the '{tool_call.function.name}' tool. Respond in a short, conversational manner, no fluff."

            tool_function_map = {
                "GenerateImageParams": self.generate_image,
                "ConvertImageParams": self.convert_image,
                "ResizeImageParams": self.resize_image,
                "OpenImageDirParams": self.open_image_directory,
            }

            if tool_call.function.name in tool_function_map:
                # 🚀 GUARANTEED OUTPUT STRUCTURE 🚀
                params = tool_call.function.parsed_arguments
                success = tool_function_map[tool_call.function.name](params)
                tool_call_success_prompt = f"Quickly let your human companion know that you've run the '{tool_call.function.name}' tool. Respond in a short, conversational manner, no fluff."
            else:
                success = False
                tool_call_success_prompt = (
                    "An unknown tool was called. Please try again."
                )

            if success:
                return prompt(self.weak_model, tool_call_success_prompt)

        else:
            # just a normal thought
            return prompt(self.weak_model, thought)




